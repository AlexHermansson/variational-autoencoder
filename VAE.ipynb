{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "MNIST handwritten digits, reshaped to vectors in $\\mathbb{R}^{784}$, and binarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], -1)) / 255\n",
    "x_train = np.where(x_train < 0.5, 0, 1).astype('float32')\n",
    "x_test  = x_test.reshape((x_test.shape[0], -1)) / 255\n",
    "x_test = np.where(x_test < 0.5, 0, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder\n",
    "\n",
    "#### Encoder\n",
    "The encoder takes an input $x$, and compresses it down to a lower dimensonal latent vector $z$. Actually, it is stochastic so it outputs parameters $\\theta = \\{\\mu, \\sigma\\}$ to $q_{\\theta}(z|x)$ which is an isotropic multivariate Gaussian distribution $\\mathcal{N}(\\mu, \\sigma I)$. When we sample from this distribution we get noisy representations of $z$.\n",
    "\n",
    "#### Decoder\n",
    "The decoder takes a latent vector $z$ and outputs parameters for the data distributions that we can sample from to generate a sample. In this case of binarized, flattened MNIST images, the output is 784 Bernoulli parameters $p_{\\phi}(x|z) = \\{ \\mu_1, \\dots, \\mu_{784} \\}$.\n",
    "\n",
    "\n",
    "#### Variational Autoencoder\n",
    "The full variational autoencoder has an encoder and a decoder. It takes a batch of input images in and outputs a batch of parameters $\\mu, \\sigma, p$ to be used to calculate the loss. $z$ is sampled using the reparameterization trick by letting $z=\\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, 1)$, to make $z$ deterministic with respect to the parameters, allowing gradients to be computed. Since the KL-diveregence regularizer pushes the parameters of the encoder toward a unit gaussian, we can then hallucinate MNIST digits by sampling from a unit gaussian (of the hidden dimension) and feeding these vectors through the decoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\" Gaussian distribution q(z|x) of dimension hidden_dim \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.h = hidden_dim\n",
    "        self.dense1 = tf.keras.layers.Dense(2 * hidden_dim, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(2 * hidden_dim)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        mean, std = x[:, :self.h], tf.nn.softplus(x[:, self.h:])\n",
    "        return mean, std\n",
    "        \n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\" Bernoulli distribution p(x|z) of dimension output_dim \"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim=784):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(2 * hidden_dim, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(output_dim, activation='sigmoid')\n",
    "    \n",
    "    def call(self, z):\n",
    "        z = self.dense1(z)\n",
    "        p = self.dense2(z)\n",
    "        return p\n",
    "        \n",
    "    \n",
    "class VAE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = Encoder(hidden_dim)\n",
    "        self.decoder = Decoder(hidden_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        mean, std = self.encoder(x)\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        z = mean + std * eps  # Reparametrization trick, so that z is deterministic wrp. mean & std. to allow gra\n",
    "        p = self.decoder(z)\n",
    "        return mean, std, p\n",
    "    \n",
    "    def hallucinate(self, n):\n",
    "        z = tf.random.normal(shape=(n, self.hidden_dim))\n",
    "        p = self.decoder(z)\n",
    "        return p.numpy().reshape(n, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "Implementing the loss functions, the reconstruction loss is a single sample Monte-Carlo estimate and the KL-divergence loss is the analytical solution between two Gaussians. One is the unit Gaussian, and the other is the distribution output by the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    \"\"\" Dodging nans \"\"\"\n",
    "    eps = 1e-8\n",
    "    return tf.math.log(x + eps)\n",
    "\n",
    "\n",
    "def reconstruction_loss(x, p):\n",
    "    return -tf.einsum(\"ij, ij -> i\", x, log(p)) - tf.einsum(\"ij, ij ->i\", (1 - x), log(1 - p))\n",
    "\n",
    "\n",
    "def kldiv_loss(mean, std):\n",
    "    return tf.reduce_sum((tf.square(std) + tf.square(mean) - 1) / 2 - log(std), axis=-1)\n",
    "\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, std, p = model(x)\n",
    "    rec_loss = reconstruction_loss(x, p)\n",
    "    kld_loss = kldiv_loss(mean, std)\n",
    "    return tf.reduce_mean(rec_loss + kld_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Just some helper functions to make the training loop a bit more readable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='training loss')\n",
    "test_loss  = tf.keras.metrics.Mean(name='test loss')\n",
    "\n",
    "def train_step(model, x):\n",
    "    with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(model, x)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "\n",
    "    \n",
    "def test_step(model, x):\n",
    "    loss = compute_loss(model, x)\n",
    "    test_loss(loss)\n",
    "\n",
    "    \n",
    "def reset_metrics():\n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "\n",
    "    \n",
    "def write_to_tensorboard(epoch):\n",
    "    tf.summary.scalar(name='training loss', data=train_loss.result(), step=epoch)\n",
    "    tf.summary.scalar(name='test loss', data=test_loss.result(), step=epoch)\n",
    "\n",
    "    \n",
    "def hallucinate_images(model, N=5, save=False):\n",
    "    \"\"\" Show N * N hallucinated images in a grid \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(N, N), axes_pad=0.1)\n",
    "    images = model.hallucinate(int(N * N))\n",
    "\n",
    "    for ax, im in zip(grid, images):\n",
    "        ax.imshow(im)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.grid(False)\n",
    "\n",
    "    plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(\"hallucinated.png\", dpi=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "LATENT = 50\n",
    "\n",
    "bs = 128\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(x_test).batch(bs)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(x_train).shuffle(10000).batch(bs)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/VAE/' + current_time\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "tmp_train = \"[{:3d}, {:3d}] ---> Train loss: {:3.2f}\"\n",
    "tmp_test = \"Test loss : {:3.2f}\\n\"\n",
    "\n",
    "model = VAE(hidden_dim=LATENT)\n",
    "opt = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "reset_metrics()\n",
    "with writer.as_default():\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        for i, x_tr in enumerate(train_ds):\n",
    "            train_step(model, x_tr)\n",
    "            if i % 100 == 0:\n",
    "                print(tmp_train.format(epoch, i, train_loss.result()))\n",
    "\n",
    "        for x_te in test_ds:\n",
    "            test_step(model, x_te)\n",
    "        \n",
    "        print(f\"After epoch {epoch}:\")\n",
    "        print(tmp_test.format(test_loss.result()))\n",
    "        \n",
    "        # Visualize some hallucinated images every 10th epoch to see the progress!\n",
    "        if epoch % 10 == 0:\n",
    "            hallucinate_images(model, 5)\n",
    "            \n",
    "        write_to_tensorboard(epoch)\n",
    "        reset_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
